{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"r8Hrl-SsHpn_"},"outputs":[],"source":["# General Libraries\n","#! pip install pandas numpy keras nltk spacy tensorflow torch\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"lZU4Y_OOHpoC"},"outputs":[],"source":["#! pip install scikit-learn"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29607,"status":"ok","timestamp":1724634580300,"user":{"displayName":"shpr lk","userId":"10059327734999523374"},"user_tz":-420},"id":"faNiqqgaIcjL","outputId":"ee591a66-6307-4dda-e356-60cea8ead5b1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":1348,"status":"ok","timestamp":1724634947317,"user":{"displayName":"shpr lk","userId":"10059327734999523374"},"user_tz":-420},"id":"QLEA4KDXOnfM"},"outputs":[],"source":["import os\n","from sklearn.metrics import f1_score, recall_score, accuracy_score\n","import numpy as np\n","import torch\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":2960,"status":"ok","timestamp":1724634951979,"user":{"displayName":"shpr lk","userId":"10059327734999523374"},"user_tz":-420},"id":"I3YIeVxjOnfY"},"outputs":[],"source":["import tensorflow as tf\n","\n","tf.keras.backend.clear_session()\n","\n","# clear gpu memory using torch\n","import torch\n","torch.cuda.empty_cache()\n","\n","# clear output\n","from IPython.display import clear_output\n","clear_output()"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":547,"status":"ok","timestamp":1724634954040,"user":{"displayName":"shpr lk","userId":"10059327734999523374"},"user_tz":-420},"id":"mNogR5VmOnfb","outputId":"b3494b39-c624-49c4-9360-cb3234a7881d"},"outputs":[{"data":{"text/plain":["device(type='cuda')"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","#device = torch.device(\"cpu\")\n","device"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":553,"status":"ok","timestamp":1724635075684,"user":{"displayName":"shpr lk","userId":"10059327734999523374"},"user_tz":-420},"id":"Y4KEsYU9Onff"},"outputs":[],"source":["test_path = (\"/content/drive/MyDrive/ViHOS/data/Sequence_labeling_based_version/Word/train_BIO_Word.csv\")"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":11430,"status":"ok","timestamp":1724634996048,"user":{"displayName":"shpr lk","userId":"10059327734999523374"},"user_tz":-420},"id":"Kxoa7w9yOnfg"},"outputs":[],"source":["from transformers import (\n","    AutoModel, AutoConfig, XLMRobertaModel,\n","    AutoTokenizer, AutoModelForSequenceClassification\n",")\n","\n","input_model = XLMRobertaModel.from_pretrained(\"xlm-roberta-base\")\n","tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n","input_model.resize_token_embeddings(len(tokenizer))\n","\n","clear_output()"]},{"cell_type":"markdown","metadata":{"id":"g3iJR318Onfi"},"source":["# Data"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":4367,"status":"ok","timestamp":1724634736368,"user":{"displayName":"shpr lk","userId":"10059327734999523374"},"user_tz":-420},"id":"jgXlU_4zOnfm"},"outputs":[],"source":["import pandas as pd\n","import transformers\n","import torch\n","import torch.nn as nn\n","import pandas as pd\n","\n","#clear output\n","from IPython.display import clear_output\n","clear_output()"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":538,"status":"ok","timestamp":1724634744315,"user":{"displayName":"shpr lk","userId":"10059327734999523374"},"user_tz":-420},"id":"cKV2F-EoOnfn"},"outputs":[],"source":["def prepare_data(file_path):\n","    df = pd.read_csv(file_path)\n","\n","    # remove nan\n","    df = df.dropna()\n","    df = df.reset_index(drop=True)\n","\n","    texts = df['Word'].tolist()\n","    spans = df['Tag'].tolist()\n","\n","    # convert spans to binary representation\n","    binary_spans = []\n","    for span in spans:\n","        binary_span = []\n","        span = span.split(' ')\n","        for s in span:\n","            if s == 'O':\n","                binary_span.append(0)\n","            else:\n","                binary_span.append(1)\n","        binary_spans.append(binary_span)\n","\n","    return texts, binary_spans"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1724634747194,"user":{"displayName":"shpr lk","userId":"10059327734999523374"},"user_tz":-420},"id":"e9iHsbPjOnfp"},"outputs":[],"source":["# Dataloader function\n","class TextDataset(torch.utils.data.Dataset):\n","    def __init__(self, texts, spans, tokenizer, max_len):\n","        self.texts = texts\n","        self.spans = spans\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        text = self.texts[idx]\n","        span = self.spans[idx]\n","\n","        # Tokenize and prepare the input data\n","        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_len, return_tensors='pt')\n","\n","        # Convert tensors to appropriate shape\n","        input_ids = encoding['input_ids'].squeeze(0)\n","        attention_mask = encoding['attention_mask'].squeeze(0)\n","\n","        return {\n","            'input_ids': input_ids,\n","            'attention_mask': attention_mask,\n","            'spans': torch.tensor(span)  # Ensure spans is converted to tensor\n","        }\n","\n","def create_dataloader(texts, spans, batch_size, tokenizer, max_len, shuffle=True):\n","    dataset = TextDataset(texts, spans, tokenizer, max_len)\n","    # return texts\n","    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n","    print(dataloader)\n","    return dataloader"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2613,"status":"ok","timestamp":1724635089149,"user":{"displayName":"shpr lk","userId":"10059327734999523374"},"user_tz":-420},"id":"w-Qyw31bOnfr","outputId":"7b90e614-9641-45bd-f416-f5cdf88ba550"},"outputs":[{"name":"stdout","output_type":"stream","text":["<torch.utils.data.dataloader.DataLoader object at 0x78c485ece320>\n","<torch.utils.data.dataloader.DataLoader object at 0x78c57064d540>\n","<torch.utils.data.dataloader.DataLoader object at 0x78c57064c3a0>\n"]}],"source":["batch_size = 32\n","test_dataloader = create_dataloader(*prepare_data(test_path), batch_size=batch_size, tokenizer = tokenizer, max_len=64)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0qk9KM_GOnft"},"outputs":[],"source":["def calculate_f1(preds, y):\n","    max_preds = preds.argmax(dim = 1, keepdim = True)\n","    return f1_score(y.cpu(), max_preds.cpu(), average='macro')"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":1717,"status":"ok","timestamp":1724634788095,"user":{"displayName":"shpr lk","userId":"10059327734999523374"},"user_tz":-420},"id":"NNxX9SOsOnfu"},"outputs":[],"source":["class MultiTaskModel(nn.Module):\n","    def __init__(self, input_model):\n","        super(MultiTaskModel, self).__init__()\n","        self.bert = input_model\n","        self.span_classifier = nn.Linear(768, 1)  \n","        self.dropout = nn.Dropout(0.1)\n","\n","    def forward(self, input_ids, attention_mask):\n","        output = self.bert(input_ids=input_ids, attention_mask=attention_mask, return_dict=False)\n","        last_hidden_state = output[0]\n","        last_hidden_state = self.dropout(last_hidden_state)\n","        span_logits = self.span_classifier(last_hidden_state)\n","\n","        span_logits = span_logits.mean(dim=1)  # Tính trung bình theo chiều 1 để giảm kích thước thành [batch_size, 1]\n","        span_logits = span_logits.unsqueeze(-1)  # Thêm chiều cuối cùng để có kích thước [batch_size, 1, 1]\n","        span_logits = torch.sigmoid(span_logits)\n","\n","        return span_logits\n"]},{"cell_type":"markdown","metadata":{"id":"p57pKpsSOnfx"},"source":["# Load and test"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28394,"status":"ok","timestamp":1724635033408,"user":{"displayName":"shpr lk","userId":"10059327734999523374"},"user_tz":-420},"id":"oQ-H4toYRxbL","outputId":"3174bd8a-af0d-40ef-8d3f-b9a78596d770"},"outputs":[{"data":{"text/plain":["MultiTaskModel(\n","  (bert): XLMRobertaModel(\n","    (embeddings): XLMRobertaEmbeddings(\n","      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n","      (position_embeddings): Embedding(514, 768, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): XLMRobertaEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x XLMRobertaLayer(\n","          (attention): XLMRobertaAttention(\n","            (self): XLMRobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): XLMRobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): XLMRobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): XLMRobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): XLMRobertaPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (span_classifier): Linear(in_features=768, out_features=1, bias=True)\n","  (dropout): Dropout(p=0.1, inplace=False)\n",")"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["# Load the model\n","device = torch.device(\"cpu\")\n","\n","model = MultiTaskModel(input_model=input_model)  # Reinitialize your model architecture\n","model.load_state_dict(torch.load(\"/content/drive/MyDrive/ViHOS/data/ViHos_40epoch.pth\", map_location=torch.device('cpu')))\n","model.to(device)  # Move the model to the appropriate device (GPU or CPU)\n","model.eval()  # Set the model to evaluation mode\n"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":776,"status":"ok","timestamp":1724635045348,"user":{"displayName":"shpr lk","userId":"10059327734999523374"},"user_tz":-420},"id":"Ob053kQR0EpL"},"outputs":[],"source":["def test(model, test_dataloader, device):\n","    model.eval()  # Set the model to evaluation mode\n","    span_preds = []\n","    span_targets = []\n","\n","    for batch in tqdm(test_dataloader):\n","        # Extract components from the batch\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        spans = batch['spans'].float().to(device)  # Ensure spans are in float for loss computation\n","\n","        with torch.no_grad():  # Disable gradient computation for inference\n","            span_logits = model(input_ids, attention_mask)  # Get model predictions\n","\n","        # Append predictions and targets for later evaluation\n","        span_preds.append(span_logits.squeeze().cpu().numpy().flatten())\n","        span_targets.append(spans.cpu().numpy().flatten())\n","\n","    # Concatenate all predictions and targets\n","    span_preds = np.concatenate(span_preds)\n","    span_targets = np.concatenate(span_targets)\n","\n","    # Binarize predictions based on threshold\n","    span_preds = (span_preds > 0.5).astype(int)\n","\n","    # Calculate metrics\n","    span_f1 = f1_score(span_targets, span_preds, average='macro')\n","    span_recall = recall_score(span_targets, span_preds, average='macro')\n","    span_accuracy = accuracy_score(span_targets, span_preds)\n","\n","    print(\"Span F1 Score: {:.4f}\".format(span_f1))\n","    print(\"Span Recall: {:.4f}\".format(span_recall))\n","    print(\"Span Accuracy: {:.4f}\".format(span_accuracy))\n"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":566,"status":"ok","timestamp":1724635101879,"user":{"displayName":"shpr lk","userId":"10059327734999523374"},"user_tz":-420},"id":"0yWUrqxr4936","outputId":"88d93626-6d89-4cd1-a77b-05d8ffec50d5"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]}],"source":["def create_subset(dataloader, subset_size=100):\n","    subset_texts = []\n","    subset_spans = []\n","\n","    for i, batch in enumerate(dataloader):\n","        if i * dataloader.batch_size >= subset_size:\n","            break\n","\n","        # Extract data based on batch structure\n","        input_ids = batch['input_ids']\n","        attention_mask = batch['attention_mask']\n","        spans = batch['spans']\n","\n","        subset_texts.append({\n","            'input_ids': input_ids,\n","            'attention_mask': attention_mask\n","        })\n","        subset_spans.append(spans)\n","\n","    # Convert lists to tensors if needed\n","    subset_texts = {\n","        'input_ids': torch.cat([x['input_ids'] for x in subset_texts], dim=0),\n","        'attention_mask': torch.cat([x['attention_mask'] for x in subset_texts], dim=0)\n","    }\n","    subset_spans = torch.cat(subset_spans, dim=0)\n","\n","    return subset_texts, subset_spans\n","\n","# Create a subset dataloader\n","def create_subset_dataloader(dataloader, subset_size=1000):\n","    subset_texts, subset_spans = create_subset(dataloader, subset_size)\n","\n","    # Define a new dataset and dataloader\n","    class SubsetDataset(torch.utils.data.Dataset):\n","        def __init__(self, texts, spans):\n","            self.texts = texts\n","            self.spans = spans\n","\n","        def __len__(self):\n","            return len(self.spans)\n","\n","        def __getitem__(self, idx):\n","            return {\n","                'input_ids': self.texts['input_ids'][idx],\n","                'attention_mask': self.texts['attention_mask'][idx],\n","                'spans': self.spans[idx]\n","            }\n","\n","    subset_dataset = SubsetDataset(subset_texts, subset_spans)\n","    subset_dataloader = torch.utils.data.DataLoader(\n","        subset_dataset, batch_size=dataloader.batch_size, shuffle=False, num_workers=4\n","    )\n","    return subset_dataloader\n","\n","# Assuming test_dataloader is defined\n","subset_dataloader = create_subset_dataloader(test_dataloader, subset_size=1000)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":113636,"status":"ok","timestamp":1724038223996,"user":{"displayName":"shpr lk","userId":"10059327734999523374"},"user_tz":-420},"id":"ucqIMCKKRzsy","outputId":"290d5881-66f9-497f-dc76-2c30f3b0cdd6"},"outputs":[{"name":"stderr","output_type":"stream","text":["\r  0%|          | 0/32 [00:00<?, ?it/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","100%|██████████| 32/32 [01:54<00:00,  3.56s/it]"]},{"name":"stdout","output_type":"stream","text":["Span F1 Score: 0.7199\n","Span Recall: 0.6888\n","Span Accuracy: 0.8564\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# Example of using the model for inference\n","test(model, subset_dataloader, device)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Sử dụng model để dự đoán từ hoặc cụm từ xấu/nhạy cảm/phân biệt,... được nhập từ người dùng"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":543,"status":"ok","timestamp":1724635289954,"user":{"displayName":"shpr lk","userId":"10059327734999523374"},"user_tz":-420},"id":"-UitYax3wEsT","outputId":"69c6cf79-cd46-492a-c55f-6fd247ac4217"},"outputs":[{"name":"stdout","output_type":"stream","text":["Input Sentence: clmn\n","Predicted Spans: 1\n"]}],"source":["sentence = \"clmn\"\n","\n","# Step 1: Tokenize the input sentence\n","encoding = tokenizer(sentence, truncation=True, padding='max_length', max_length=64, return_tensors='pt')\n","\n","# Step 2: Prepare input tensors\n","input_ids = encoding['input_ids'].to(device)\n","attention_mask = encoding['attention_mask'].to(device)\n","\n","# Step 3: Pass the tensors through the model\n","model.eval()  # Set the model to evaluation mode\n","with torch.no_grad():\n","    output = model(input_ids, attention_mask)\n","\n","# Step 4: Interpret the output\n","# Assuming the model predicts binary spans (0 or 1)\n","span_logits = output.squeeze().cpu().numpy()\n","span_predictions = (span_logits > 0.5).astype(int)  # Convert probabilities to binary predictions\n","\n","# Print the results\n","print(\"Input Sentence:\", sentence)\n","print(\"Predicted Spans:\", span_predictions)\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"2ea4058ebff17680e9aef3310abdc85a026daf30b518fcd609e9534b2c19ba1f"}}},"nbformat":4,"nbformat_minor":0}
