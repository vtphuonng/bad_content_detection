{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Model should be trained multiple times with different random seeds to get the best model."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":41483,"status":"ok","timestamp":1641748295070,"user":{"displayName":"Nap Lan Dau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgyLJ7xblBnvLEyFishN4Q4xZSVXTxeaLzYRJU=s64","userId":"01595996840706054717"},"user_tz":-420},"id":"lqCp3rbqNGnO","outputId":"6567c635-78f2-4f41-c753-973e1995d8a9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting tensorflow-gpu==1.15.0\n","  Downloading tensorflow_gpu-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (411.5 MB)\n","\u001b[K     |████████████████████████████████| 411.5 MB 7.2 kB/s \n","\u001b[?25hCollecting keras-applications>=1.0.8\n","  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n","\u001b[K     |████████████████████████████████| 50 kB 8.9 MB/s \n","\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (1.13.3)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (1.42.0)\n","Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (0.2.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (3.3.0)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (3.17.3)\n","Collecting tensorflow-estimator==1.15.1\n","  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n","\u001b[K     |████████████████████████████████| 503 kB 96.1 MB/s \n","\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (0.12.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (0.37.0)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (0.8.1)\n","Collecting tensorboard<1.16.0,>=1.15.0\n","  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n","\u001b[K     |████████████████████████████████| 3.8 MB 70.2 MB/s \n","\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (1.15.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (1.1.0)\n","Collecting gast==0.2.2\n","  Downloading gast-0.2.2.tar.gz (10 kB)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (1.1.2)\n","Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15.0) (1.19.5)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==1.15.0) (3.1.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (57.4.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (1.0.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (3.3.6)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (4.8.2)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (3.10.0.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15.0) (3.6.0)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow-gpu==1.15.0) (1.5.2)\n","Building wheels for collected packages: gast\n","  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=1b93f5427596dc4368671f3a698e630809f98ee3d4e6fd038d40466644292341\n","  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n","Successfully built gast\n","Installing collected packages: tensorflow-estimator, tensorboard, keras-applications, gast, tensorflow-gpu\n","  Attempting uninstall: tensorflow-estimator\n","    Found existing installation: tensorflow-estimator 2.7.0\n","    Uninstalling tensorflow-estimator-2.7.0:\n","      Successfully uninstalled tensorflow-estimator-2.7.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.7.0\n","    Uninstalling tensorboard-2.7.0:\n","      Successfully uninstalled tensorboard-2.7.0\n","  Attempting uninstall: gast\n","    Found existing installation: gast 0.4.0\n","    Uninstalling gast-0.4.0:\n","      Successfully uninstalled gast-0.4.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.7.0 requires tensorboard~=2.6, but you have tensorboard 1.15.0 which is incompatible.\n","tensorflow 2.7.0 requires tensorflow-estimator<2.8,~=2.7.0rc0, but you have tensorflow-estimator 1.15.1 which is incompatible.\n","tensorflow-probability 0.15.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\u001b[0m\n","Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-estimator-1.15.1 tensorflow-gpu-1.15.0\n"]}],"source":["pip install tensorflow-gpu==1.15.0"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3075,"status":"ok","timestamp":1641748298138,"user":{"displayName":"Nap Lan Dau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgyLJ7xblBnvLEyFishN4Q4xZSVXTxeaLzYRJU=s64","userId":"01595996840706054717"},"user_tz":-420},"id":"7MfrOvXwNqdM","outputId":"21a63bf2-0f43-4464-9dd1-4e90a19f3902"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting keras==2.2.4\n","  Downloading Keras-2.2.4-py2.py3-none-any.whl (312 kB)\n","\u001b[?25l\r\u001b[K     |█                               | 10 kB 37.5 MB/s eta 0:00:01\r\u001b[K     |██                              | 20 kB 29.9 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 30 kB 19.8 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 40 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 51 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 61 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 71 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 81 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 92 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 102 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 112 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 122 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 133 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 143 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 153 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 163 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 174 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 184 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 194 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 204 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 215 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 225 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 235 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 245 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 256 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 266 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 276 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 286 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 296 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 307 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 312 kB 8.2 MB/s \n","\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (3.13)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.0.8)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (3.1.0)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.4.1)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.1.2)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.19.5)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.15.0)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras==2.2.4) (1.5.2)\n","Installing collected packages: keras\n","  Attempting uninstall: keras\n","    Found existing installation: keras 2.7.0\n","    Uninstalling keras-2.7.0:\n","      Successfully uninstalled keras-2.7.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.7.0 requires keras<2.8,>=2.7.0rc0, but you have keras 2.2.4 which is incompatible.\n","tensorflow 2.7.0 requires tensorboard~=2.6, but you have tensorboard 1.15.0 which is incompatible.\n","tensorflow 2.7.0 requires tensorflow-estimator<2.8,~=2.7.0rc0, but you have tensorflow-estimator 1.15.1 which is incompatible.\u001b[0m\n","Successfully installed keras-2.2.4\n"]}],"source":["!pip install keras==2.2.4"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4412,"status":"ok","timestamp":1641748302546,"user":{"displayName":"Nap Lan Dau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgyLJ7xblBnvLEyFishN4Q4xZSVXTxeaLzYRJU=s64","userId":"01595996840706054717"},"user_tz":-420},"id":"-KutAPa4Nvs5","outputId":"c1f2249b-d562-47e4-aac6-6c962b34e5c4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting git+https://www.github.com/keras-team/keras-contrib.git\n","  Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-xiba1f3l\n","  Running command git clone -q https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-xiba1f3l\n","Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from keras-contrib==2.0.8) (2.2.4)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.1.2)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (3.13)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.19.5)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.15.0)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.4.1)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (3.1.0)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from keras->keras-contrib==2.0.8) (1.0.8)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras->keras-contrib==2.0.8) (1.5.2)\n","Building wheels for collected packages: keras-contrib\n","  Building wheel for keras-contrib (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-contrib: filename=keras_contrib-2.0.8-py3-none-any.whl size=101077 sha256=918f218497b8f67389c6e9106991e779720ceeff165037703381a93d3a366ae4\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-5rt_zec8/wheels/bb/1f/f2/b57495012683b6b20bbae94a3915ec79753111452d79886abc\n","Successfully built keras-contrib\n","Installing collected packages: keras-contrib\n","Successfully installed keras-contrib-2.0.8\n"]}],"source":["!pip install git+https://www.github.com/keras-team/keras-contrib.git"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":85610,"status":"ok","timestamp":1641748388148,"user":{"displayName":"Nap Lan Dau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgyLJ7xblBnvLEyFishN4Q4xZSVXTxeaLzYRJU=s64","userId":"01595996840706054717"},"user_tz":-420},"id":"0df7fmkYTUOo","outputId":"46752bdf-200f-4304-e928-44cf4f713d39"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"T5h8vjpOOA66"},"source":["# LOAD DATA"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2848,"status":"ok","timestamp":1641748390990,"user":{"displayName":"Nap Lan Dau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgyLJ7xblBnvLEyFishN4Q4xZSVXTxeaLzYRJU=s64","userId":"01595996840706054717"},"user_tz":-420},"id":"hkXzai4kOCAW","outputId":"dc98305b-d417-43b8-f347-22e3a152c9b2"},"outputs":[{"name":"stderr","output_type":"stream","text":["Using TensorFlow backend.\n"]}],"source":["from tensorflow import keras \n","import pandas as pd\n","import numpy as np\n","from ast import literal_eval\n","from nltk.tokenize import TweetTokenizer\n","import spacy\n","from copy import deepcopy\n","from keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.utils import to_categorical\n","from keras.initializers import Constant\n","from nltk.corpus import stopwords"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1641748390990,"user":{"displayName":"Nap Lan Dau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgyLJ7xblBnvLEyFishN4Q4xZSVXTxeaLzYRJU=s64","userId":"01595996840706054717"},"user_tz":-420},"id":"1fx7ahrmUHml","outputId":"9594175c-40c5-419f-e246-d01e3e9a401f"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n","  \n"]}],"source":["from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\"\n","\n","\n","pd.set_option('display.max_rows', None)\n","pd.set_option('display.max_columns', None)\n","pd.set_option('display.width', None)\n","pd.set_option('display.max_colwidth', -1)\n","pd.options.display.max_rows"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EXqKe1GiN97b"},"outputs":[],"source":["\n","data = pd.read_csv('train.csv')\n","test = pd.read_csv('test.csv')\n","dev = pd.read_csv('dev.csv')\n","\n","text_data = data['content'].values\n","spans_data = data['index_spans'].apply(literal_eval)\n","lbl_data = [1 if len(s) > 0 else 0 for s in spans_data]\n","\n","text_test = test['content'].values\n","spans_test = test['index_spans'].apply(literal_eval)\n","lbl_test = [1 if len(s) > 0 else 0 for s in spans_test]\n","\n","text_dev = dev['content'].values\n","spans_dev = dev['index_spans'].apply(literal_eval)\n","lbl_dev = [1 if len(s) > 0 else 0 for s in spans_dev]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sQk8yXbaPHPI"},"outputs":[],"source":["tokenizer = TweetTokenizer()\n","\n","def tokenize_text(text):\n","  return tokenizer.tokenize(text)\n","\n","def toxic_word(span, text):\n","  i = 0\n","  token = []\n","  a = 0\n","  word = []\n","\n","  while (i < (len(span) - 1)):\n","      if (span[i] != (span[i+1]-1)):\n","          token.append(span[a:(i+1)])\n","          a = i + 1\n","      elif i == (len(span) - 2):\n","          token.append(span[a:i+2])\n","      i = i + 1\n","  for t in token:\n","      word.append(text[t[0]:(t[len(t)-1])+1])\n","  return word\n","def span_convert(text_data, spans):\n","    MAX_LEN = 0\n","    token_labels = []\n","\n","    for i in range(0, len(text_data)):\n","        token_labels.append(toxic_word(spans[i], text_data[i]))\n","\n","    lst_seq = []\n","    for i in range(0, len(text_data)):\n","        token = tokenize_text(text_data[i])\n","        if len(token) > MAX_LEN:\n","            MAX_LEN = len(token)\n","            \n","        seq = np.zeros(len(token), dtype=int)\n","        for j in range(0, len(token)):\n","            for t in token_labels[i]:\n","                if token[j] in tokenize_text(t):\n","                    seq[j] = 1\n","        lst_seq.append(seq)     \n","\n","    return (token_labels, lst_seq)\n","def span_retrived(text_data, spans):\n","    token_labels = []\n","\n","    for i in range(0, len(text_data)):\n","        token_labels.append(toxic_word(spans[i], text_data[i]))\n","    \n","    return token_labels\n","def decoding(text_data, encoding_text, prediction):\n","    test = [[idx2word[i] for i in row] for row in encoding_text]\n","\n","    lst_token = []\n","\n","    for t in range(0, len(test)):\n","        yy_pred = []\n","        for i in range(0, len(test[t])):\n","            if prediction[t][i] == 1:\n","                yy_pred.append(test[t][i])\n","        lst_token.append(yy_pred)\n","\n","    lis_idx = []\n","    for i in range(0, len(text_data)):\n","        idx = []\n","        for t in lst_token[i]:\n","            index = text_data[i].find(t)\n","            idx.append(index)\n","            for j in range(1, len(t)):\n","                index = index + 1\n","                idx.append(index)\n","        lis_idx.append(idx)\n","\n","    return lis_idx"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":546},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1641748394534,"user":{"displayName":"Nap Lan Dau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgyLJ7xblBnvLEyFishN4Q4xZSVXTxeaLzYRJU=s64","userId":"01595996840706054717"},"user_tz":-420},"id":"ZN7bRD9KQBkJ","outputId":"0b5ffb01-0e3f-43ad-ed1b-43b19da2819d"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-844d1c41-6202-41f4-bb58-6cc62a3ae3ce\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>level_0</th>\n","      <th>index</th>\n","      <th>content</th>\n","      <th>index_spans</th>\n","      <th>token</th>\n","      <th>seq</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>2051</td>\n","      <td>104</td>\n","      <td>À thế bạn ý hát bài j jui có ai giới thiệu cho mình đc ko. Lâu jui mình chẳng nghe bài hát mới nào cả</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>2087</td>\n","      <td>140</td>\n","      <td>Thầy đang nghĩ về cô em xinh tươi này ...</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>2357</td>\n","      <td>410</td>\n","      <td>Ủa tứ quý có chặt đôi heo đc đâu, lộ tứ quý rồi nhaaa hahhahaha t cho m thúi Trần Thị Linh Ánh Bui Thi My Ngoc Thanh Hà Thanh Thảo Thùy Trang Lê Thị Oanh Hoàng Linhc</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>2588</td>\n","      <td>641</td>\n","      <td>tốt, cứ giá cao đi -_- chứ bh nông dân cũng mua đc xe thì k biết đường nó còn ra cái gì</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>3854</td>\n","      <td>1907</td>\n","      <td>Mấy cái quan bệnh tật của thời đại</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>5</td>\n","      <td>2565</td>\n","      <td>618</td>\n","      <td>Mặt nó lại ngáo 🤣</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[0, 0, 0, 0, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>6</td>\n","      <td>617</td>\n","      <td>617</td>\n","      <td>Ngu cmm.</td>\n","      <td>[0, 1, 2, 3, 4, 5, 6]</td>\n","      <td>[Ngu cmm]</td>\n","      <td>[1, 1, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>7</td>\n","      <td>918</td>\n","      <td>918</td>\n","      <td>ai coi trần dần và ngô kỷ là kẻ ngu ngóc...</td>\n","      <td>[29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39]</td>\n","      <td>[kẻ ngu ngóc]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>8</td>\n","      <td>2118</td>\n","      <td>171</td>\n","      <td>Nhưng mà khi tỏ tình xong rồi bất bại vừa mất ng mình thương vừa mất luôn bạn 🙂</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>9</td>\n","      <td>3170</td>\n","      <td>1223</td>\n","      <td>tộc caaaa</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","      <td>[0, 0]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-844d1c41-6202-41f4-bb58-6cc62a3ae3ce')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-844d1c41-6202-41f4-bb58-6cc62a3ae3ce button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-844d1c41-6202-41f4-bb58-6cc62a3ae3ce');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["   Unnamed: 0  level_0  index  \\\n","0  0           2051     104     \n","1  1           2087     140     \n","2  2           2357     410     \n","3  3           2588     641     \n","4  4           3854     1907    \n","5  5           2565     618     \n","6  6           617      617     \n","7  7           918      918     \n","8  8           2118     171     \n","9  9           3170     1223    \n","\n","                                                                                                                                                                 content  \\\n","0  À thế bạn ý hát bài j jui có ai giới thiệu cho mình đc ko. Lâu jui mình chẳng nghe bài hát mới nào cả                                                                   \n","1  Thầy đang nghĩ về cô em xinh tươi này ...                                                                                                                               \n","2  Ủa tứ quý có chặt đôi heo đc đâu, lộ tứ quý rồi nhaaa hahhahaha t cho m thúi Trần Thị Linh Ánh Bui Thi My Ngoc Thanh Hà Thanh Thảo Thùy Trang Lê Thị Oanh Hoàng Linhc   \n","3  tốt, cứ giá cao đi -_- chứ bh nông dân cũng mua đc xe thì k biết đường nó còn ra cái gì                                                                                 \n","4  Mấy cái quan bệnh tật của thời đại                                                                                                                                      \n","5  Mặt nó lại ngáo 🤣                                                                                                                                                       \n","6  Ngu cmm.                                                                                                                                                                \n","7  ai coi trần dần và ngô kỷ là kẻ ngu ngóc...                                                                                                                             \n","8  Nhưng mà khi tỏ tình xong rồi bất bại vừa mất ng mình thương vừa mất luôn bạn 🙂                                                                                         \n","9  tộc caaaa                                                                                                                                                               \n","\n","                                    index_spans          token  \\\n","0  []                                            []              \n","1  []                                            []              \n","2  []                                            []              \n","3  []                                            []              \n","4  []                                            []              \n","5  []                                            []              \n","6  [0, 1, 2, 3, 4, 5, 6]                         [Ngu cmm]       \n","7  [29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39]  [kẻ ngu ngóc]   \n","8  []                                            []              \n","9  []                                            []              \n","\n","                                                                                                                     seq  \n","0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]                                      \n","1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]                                                                                         \n","2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n","3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]                                         \n","4  [0, 0, 0, 0, 0, 0, 0, 0]                                                                                               \n","5  [0, 0, 0, 0, 0]                                                                                                        \n","6  [1, 1, 0]                                                                                                              \n","7  [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0]                                                                                   \n","8  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]                                                              \n","9  [0, 0]                                                                                                                 "]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["data['token'], data['seq'] =  span_convert(text_data, spans_data)\n","dev['token'], dev['seq'] =  span_convert(text_dev, spans_dev)\n","test['token'], test['seq'] =  span_convert(text_test, spans_test)\n","data.head(10)\n","train = deepcopy(data)"]},{"cell_type":"markdown","metadata":{"id":"S8PofulBbdK3"},"source":["# EVALUATION METRIC"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Aijjzh9-XluP"},"outputs":[],"source":["import sys\n","import os\n","import os.path\n","from scipy.stats import sem\n","import numpy as np\n","from ast import literal_eval\n","\n","def f1(predictions, gold):\n","    \"\"\"\n","    F1 (a.k.a. DICE) operating on two lists of offsets (e.g., character).\n","    >>> assert f1([0, 1, 4, 5], [0, 1, 6]) == 0.5714285714285714\n","    :param predictions: a list of predicted offsets\n","    :param gold: a list of offsets serving as the ground truth\n","    :return: a score between 0 and 1\n","    \"\"\"\n","    if len(gold) == 0:\n","        return 1. if len(predictions) == 0 else 0.\n","    if len(predictions) == 0:\n","        return 0.\n","    predictions_set = set(predictions)\n","    gold_set = set(gold)\n","    nom = 2 * len(predictions_set.intersection(gold_set))\n","    denom = len(predictions_set) + len(gold_set)\n","    return float(nom)/float(denom)\n","\n","\n","def evaluate(pred, gold):\n","    \"\"\"\n","    Based on https://github.com/felipebravom/EmoInt/blob/master/codalab/scoring_program/evaluation.py\n","    :param pred: file with predictions\n","    :param gold: file with ground truth\n","    :return:\n","    \"\"\"\n","    # # read the predictions\n","    # pred_lines = pred.readlines()\n","    # # read the ground truth\n","    # gold_lines = gold.readlines()\n","\n","    pred_lines = pred\n","    gold_lines = gold\n","\n","    # only when the same number of lines exists\n","    if (len(pred_lines) == len(gold_lines)):\n","        data_dic = {}\n","        for n, line in enumerate(gold_lines):\n","            parts = line.split('\\t')\n","            if len(parts) == 2:\n","                data_dic[int(parts[0])] = [literal_eval(parts[1])]\n","            else:\n","                raise ValueError('Format problem for gold line %d.', n)\n","\n","        for n, line in enumerate(pred_lines):\n","            parts = line.split('\\t')\n","            if len(parts) == 2:\n","                if int(parts[0]) in data_dic:\n","                    try:\n","                        data_dic[int(parts[0])].append(literal_eval(parts[1]))\n","                    except ValueError:\n","                        # Invalid predictions are replaced by a default value\n","                        data_dic[int(parts[0])].append([])\n","                else:\n","                    raise ValueError('Invalid text id for pred line %d.', n)\n","            else:\n","                raise ValueError('Format problem for pred line %d.', n)\n","\n","        # lists storing gold and prediction scores\n","        scores = []\n","        for id in data_dic:\n","            if len(data_dic[id]) == 2:\n","                gold_spans = data_dic[id][0]\n","                pred_spans = data_dic[id][1]\n","                scores.append(f1(pred_spans, gold_spans))\n","            else:\n","                sys.exit('Repeated id in test data.')\n","\n","        return (np.mean(scores), sem(scores))\n"]},{"cell_type":"markdown","metadata":{"id":"Uo9Rs5A3zAa8"},"source":["# Word Embedding"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":45651,"status":"ok","timestamp":1641748440177,"user":{"displayName":"Nap Lan Dau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgyLJ7xblBnvLEyFishN4Q4xZSVXTxeaLzYRJU=s64","userId":"01595996840706054717"},"user_tz":-420},"id":"kXeR0uQGQaxF","outputId":"a661a44a-8941-4fdf-b02a-90e9d0f6907a"},"outputs":[{"name":"stdout","output_type":"stream","text":["GloVe data loaded\n"]}],"source":["# Read embedding\n","word_dict = []\n","embeddings_index = {}\n","f = open('word2vec_vi_words_100dims.txt')\n","for line in f:\n","    values = line.split(' ')\n","    word = values[0] \n","    word_dict.append(word)\n","    coefs = np.asarray(values[1:], dtype='float32')\n","    embeddings_index[word] = coefs\n","f.close()\n","\n","print('GloVe data loaded')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g09huNrtzD41"},"outputs":[],"source":["words = word_dict\n","num_words = len(words)\n","\n","# Dictionary word:index pair\n","# word is key and its value is corresponding index\n","word_to_index = {w : i + 2 for i, w in enumerate(words)}\n","word_to_index[\"UNK\"] = 1\n","word_to_index[\"PAD\"] = 0\n","\n","# Dictionary lable:index pair\n","idx2word = {i: w for w, i in word_to_index.items()}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d3qi1TZZzHMp"},"outputs":[],"source":["# first create a matrix of zeros, this is our embedding matrix\n","embedding_dim = 100\n","max_len = 10\n","max_feature = 1000\n","embedding_matrix = np.zeros((num_words, embedding_dim))\n","\n","# for each word in out tokenizer lets try to find that work in our w2v model\n","for word, i in word_to_index.items():\n","    if i > max_feature:\n","        continue\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","        # we found the word - add that words vector to the matrix\n","        embedding_matrix[i] = embedding_vector\n","    else:\n","        # doesn't exist, assign a random vector\n","        embedding_matrix[i] = np.random.randn(embedding_dim)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oxUF87qWzImk"},"outputs":[],"source":[" # mapping for token cases\n","case2Idx = {'1': 1, '0': 0}\n","caseEmbeddings = np.identity(len(case2Idx), dtype='float32')  # identity matrix used \n","\n","char2Idx = {\"PADDING\": 0, \"UNKNOWN\": 1}\n","for c in \" 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ.,-_()[]{}!?:;#'\\\"/\\\\%$`&=*+@^~|<>\":\n","    char2Idx[c] = len(char2Idx)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EPg9dMti68ey"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","y = data['seq']\n","X = data['content']\n","\n","y_test = test['seq']\n","X_test = test['content']\n","\n","y_dev = dev['seq']\n","X_dev = dev['content']\n","\n","X_train = X\n","y_train = y"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DtgToomq_PGl"},"outputs":[],"source":["tknzr2 = TweetTokenizer()\n","\n","def custom_tokenizer(text_data):\n","    text_data = text_data.lower()\n","    return tknzr2.tokenize(text_data)\n","\n","def encoding(X, y, isTest = True):\n","    sentences = []\n","    \n","    for t in X:\n","        sentences.append(custom_tokenizer(t))\n","\n","    X = []\n","    for s in sentences:\n","        sent = []\n","        for w in s:\n","            try:\n","                w = w.lower()\n","                sent.append(word_to_index[w])\n","            except:\n","                sent.append(word_to_index[\"UNK\"])\n","        X.append(sent)\n","           \n","    X = pad_sequences(maxlen = max_len, sequences = X, padding = \"post\", value = word_to_index[\"PAD\"])\n","\n","    if isTest:\n","        y = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=word_to_index[\"PAD\"])\n","        y = to_categorical(y, num_classes=2)\n","    else:\n","        y = None\n","\n","    return (X,y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vCl6EvPjAavV"},"outputs":[],"source":["X1, y1 = encoding(X_train, y_train)\n","X2, y2 = encoding(X_dev, y_dev)\n","X3, y3 = encoding(X_test, y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":41485,"status":"ok","timestamp":1641748483897,"user":{"displayName":"Nap Lan Dau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgyLJ7xblBnvLEyFishN4Q4xZSVXTxeaLzYRJU=s64","userId":"01595996840706054717"},"user_tz":-420},"id":"Cjnn2tguAdY8","outputId":"09d851ad-7581-4178-c953-dee5c69443ec"},"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/math_ops.py:2509: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_1 (InputLayer)         (None, 10)                0         \n","_________________________________________________________________\n","embedding_1 (Embedding)      (None, 10, 100)           158751000 \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 10, 100)           0         \n","_________________________________________________________________\n","bidirectional_1 (Bidirection (None, 10, 20)            8880      \n","_________________________________________________________________\n","time_distributed_1 (TimeDist (None, 10, 10)            210       \n","_________________________________________________________________\n","crf_1 (CRF)                  (None, 10, 2)             30        \n","=================================================================\n","Total params: 158,760,120\n","Trainable params: 158,760,120\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["# BiLSTM - CRF \n","from keras.layers import LSTM, Dense, TimeDistributed, Embedding, Bidirectional, Flatten, Dropout\n","\n","from keras.models import Model, Input\n","from keras_contrib.layers import CRF\n","from keras.utils.vis_utils import plot_model\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","# from keras.metrics import BinaryAccuracy, Precision, Recall, AUC\n","\n","input = Input(shape = (max_len,))\n","model = Embedding(input_dim=num_words+2,\n","                    output_dim=embedding_dim,\n","                    embeddings_initializer=Constant(embedding_matrix),\n","                    input_length=max_len,\n","                    trainable=True)(input)\n","\n","model = Dropout(0.1)(model)\n","model = Bidirectional(LSTM(units = max_len, return_sequences=True, recurrent_dropout=0.1))(model)\n","model = TimeDistributed(Dense(max_len, activation=\"relu\"))(model)\n","crf = CRF(2)  \n","out = crf(model)  # output\n","\n","model = Model(input, out)\n","model.compile(optimizer=\"adam\", loss=crf.loss_function, metrics=['accuracy'])\n","\n","model.summary()\n","\n","plot_model(model,to_file=\"bilstm-crf.pdf\",show_shapes=True,show_layer_names=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":252395,"status":"ok","timestamp":1641749165477,"user":{"displayName":"Nap Lan Dau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgyLJ7xblBnvLEyFishN4Q4xZSVXTxeaLzYRJU=s64","userId":"01595996840706054717"},"user_tz":-420},"id":"EInItV9iESG_","outputId":"4e916520-74be-4987-a6ac-7a5fd7183dd7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train on 3894 samples, validate on 488 samples\n","Epoch 1/4\n","3894/3894 [==============================] - 1s 177us/step - loss: 0.5223 - acc: 0.8888 - val_loss: 0.3817 - val_acc: 0.5721\n","Epoch 2/4\n","3894/3894 [==============================] - 1s 181us/step - loss: 0.3129 - acc: 0.8888 - val_loss: 0.2514 - val_acc: 0.8525\n","Epoch 3/4\n","3894/3894 [==============================] - 1s 172us/step - loss: 0.2206 - acc: 0.8888 - val_loss: 0.2151 - val_acc: 0.8857\n","Epoch 4/4\n","3894/3894 [==============================] - 1s 176us/step - loss: 0.1961 - acc: 0.8888 - val_loss: 0.2073 - val_acc: 0.8850\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x7f73743f6310>"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["from keras.callbacks import ModelCheckpoint\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","checkpointer = ModelCheckpoint(filepath = 'model_detection_19_word.h5',\n","                       verbose = 0,\n","                       mode = 'auto',\n","                       save_best_only = True,\n","                       monitor='val_loss')\n","\n","model.fit(X1, np.array(y1), batch_size=512, epochs=4, validation_data=(X2, y2), callbacks=[checkpointer])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3yK1M8yrEMEu"},"outputs":[],"source":["y_pred = model.predict(X3)\n","y_pred = np.argmax(y_pred, axis=-1)\n","y_test_true = np.argmax(y3, -1)\n","# y_pred = [[i for i in row] for row in y_pred]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PpFtuOjBCLOJ"},"outputs":[],"source":["raw_y = decoding(X_test, X3, y_pred)\n"]},{"cell_type":"markdown","metadata":{"id":"crS4RoxuQBfa"},"source":["error df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ycS2-kuKMuoQ"},"outputs":[],"source":["token_predict, seq_predict = span_convert(text_test, raw_y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3yBjPKWFQAvR"},"outputs":[],"source":["error = pd.DataFrame()\n","error['True'] = test['token']\n","error['Pred'] = token_predict"]},{"cell_type":"markdown","metadata":{"id":"0lnQHCAIzzqb"},"source":["# PREDICT"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SRQ3jz39zy_m"},"outputs":[],"source":["def tokenize(text, pos):\n","    tokens = text.split()\n","    alignment = []\n","    start = 0\n","    for t in tokens:\n","        res = text.find(t, start)\n","        alignment.append(pos[res:res + len(t)])\n","        start = res + len(t)\n","    assert len(tokens) == len(alignment)\n","    return tokens, alignment\n","def y_true(data):\n","  index_true = []\n","  data['index_spans'] = data['index_spans'].apply(literal_eval)\n","  for i in range(len(data)):\n","    text = data['content'][i]\n","    pos = [i for i in range(len(text))]\n","    df_point = pd.DataFrame()\n","    df_point['spans'] = pos\n","    df_point['spans'] = 0\n","    if not data['index_spans'][i]:\n","      index_true.append(list(df_point['spans']))\n","    else:\n","      for j in data['index_spans'][i]:\n","        df_point['spans'][j] = 1\n","      index_true.append(list(df_point['spans']))\n","  return index_true\n","def y_pred(test, error):\n","  index_pred = []\n","  for dt in range(len(error)):\n","    value_predict_i = error['Pred'][dt]\n","    text = test['content'][dt]\n","    pos = [i for i in range(len(text))]\n","    tokens, alignment = tokenize(text, pos)\n","    df_point = pd.DataFrame()\n","    df_point['spans'] = pos\n","    df_point['spans'] = 0\n","    for j in range(len(tokens)):\n","      if tokens[j] in value_predict_i:\n","        for ali in alignment[j]:\n","            df_point['spans'][ali] = 1\n","    index_pred.append(list(df_point['spans']))\n","  return index_pred\n","true = y_true(test)\n","pred = y_pred(test, error)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7h-ng6cx8pay"},"outputs":[],"source":["from sklearn.metrics import precision_recall_fscore_support\n","scores_f1_macro = []\n","scores_f1_micro = []\n","scores_precision_macro = []\n","scores_precision_micro = []\n","scores_recall_macro = []\n","scores_recall_micro = []\n","\n","for i in range(len(true)):\n","  score_macro = precision_recall_fscore_support(true[i], pred[i], average='macro')\n","  score_micro = precision_recall_fscore_support(true[i], pred[i], average='micro')\n","\n","  scores_f1_macro.append(score_macro[2])\n","  scores_f1_micro.append(score_micro[2])\n","  scores_precision_macro.append(score_macro[0])\n","  scores_precision_micro.append(score_micro[0])\n","  scores_recall_macro.append(score_macro[1])\n","  scores_recall_micro.append(score_micro[1])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CHfFUCwz8wbY"},"outputs":[],"source":["scores = pd.DataFrame()\n","scores['F1-micro'] = [np.mean(scores_f1_micro)]\n","scores['F1-macro'] = [np.mean(scores_f1_macro)]\n","scores['Precision-macro'] = [np.mean(scores_precision_macro)]\n","scores['Precision-micro'] = [np.mean(scores_precision_micro)]\n","scores['Recall-macro'] = [np.mean(scores_recall_macro)]\n","scores['Recall-micro'] = [np.mean(scores_recall_micro)]"]},{"cell_type":"markdown","metadata":{"id":"f6tVYPVA80M2"},"source":["# Save"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0vwn1rza9bB3"},"outputs":[],"source":["scores.to_csv('score.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b64ikatF9eFg"},"outputs":[],"source":["error.to_csv('error.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b5Vq4lBD_np7"},"outputs":[],"source":["model.save('model.h5')\n"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"},"vscode":{"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"}}},"nbformat":4,"nbformat_minor":0}
