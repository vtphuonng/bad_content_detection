{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: pandas in /home/lkshpr/Yootek/bad_word_detection/ViHOS/EnvVihos/lib/python3.10/site-packages (2.2.2)\n","Requirement already satisfied: numpy in /home/lkshpr/Yootek/bad_word_detection/ViHOS/EnvVihos/lib/python3.10/site-packages (1.26.4)\n","Requirement already satisfied: tzdata>=2022.7 in /home/lkshpr/Yootek/bad_word_detection/ViHOS/EnvVihos/lib/python3.10/site-packages (from pandas) (2024.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /home/lkshpr/Yootek/bad_word_detection/ViHOS/EnvVihos/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /home/lkshpr/Yootek/bad_word_detection/ViHOS/EnvVihos/lib/python3.10/site-packages (from pandas) (2024.1)\n","Requirement already satisfied: six>=1.5 in /home/lkshpr/Yootek/bad_word_detection/ViHOS/EnvVihos/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"]}],"source":["! pip install pandas numpy"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"_KFIOHagGKbc"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from ast import literal_eval\n","import re\n","from ast import literal_eval"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4358,"status":"ok","timestamp":1641556649092,"user":{"displayName":"Nap Lan Dau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgyLJ7xblBnvLEyFishN4Q4xZSVXTxeaLzYRJU=s64","userId":"01595996840706054717"},"user_tz":-420},"id":"AsYLPNS2I9P-","outputId":"76891995-d147-4646-aebf-b245ee73334e"},"outputs":[],"source":["# Word segmenter\n","!pip3 install vncorenlp\n","\n","# Download VnCoreNLP-1.1.1.jar & its word segmentation component (i.e. RDRSegmenter) \n","!mkdir -p vncorenlp/models/wordsegmenter\n","!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/VnCoreNLP-1.1.1.jar\n","!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/vi-vocab\n","!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/wordsegmenter.rdr\n","!mv VnCoreNLP-1.1.1.jar vncorenlp/ \n","!mv vi-vocab vncorenlp/models/wordsegmenter/\n","!mv wordsegmenter.rdr vncorenlp/models/wordsegmenter/"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24,"status":"ok","timestamp":1641556649094,"user":{"displayName":"Nap Lan Dau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgyLJ7xblBnvLEyFishN4Q4xZSVXTxeaLzYRJU=s64","userId":"01595996840706054717"},"user_tz":-420},"id":"BSYQ85ybG7Hg","outputId":"e65ce208-1ccf-41ef-8e0b-d8b2a588b3c2"},"outputs":[],"source":["from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\"\n","\n","\n","pd.set_option('display.max_rows', None)\n","pd.set_option('display.max_columns', None)\n","pd.set_option('display.width', None)\n","pd.set_option('display.max_colwidth', None)\n","pd.options.display.max_rows"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":191},"executionInfo":{"elapsed":341,"status":"ok","timestamp":1641570592726,"user":{"displayName":"Nap Lan Dau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgyLJ7xblBnvLEyFishN4Q4xZSVXTxeaLzYRJU=s64","userId":"01595996840706054717"},"user_tz":-420},"id":"fn-nOSPGHMGw","outputId":"e51b26de-f1e2-43c7-dce2-05f35f416294"},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'train.csv'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m dev \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdev.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m test \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","File \u001b[0;32m~/Yootek/bad_word_detection/ViHOS/EnvVihos/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Yootek/bad_word_detection/ViHOS/EnvVihos/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n","File \u001b[0;32m~/Yootek/bad_word_detection/ViHOS/EnvVihos/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Yootek/bad_word_detection/ViHOS/EnvVihos/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n","File \u001b[0;32m~/Yootek/bad_word_detection/ViHOS/EnvVihos/lib/python3.10/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train.csv'"]}],"source":["train = pd.read_csv('train.csv')\n","dev = pd.read_csv('dev.csv')\n","test = pd.read_csv('test.csv')\n","test['index_spans'] = test['index_spans'].apply(literal_eval)\n","train['index_spans'] = train['index_spans'].apply(literal_eval)\n","dev['index_spans'] = dev['index_spans'].apply(literal_eval)\n","\n","headers = ['Unnamed: 0',  'content', 'index_spans', 'Span']\n","train.columns = headers\n","dev.columns = headers\n","test.columns = headers\n","test.head(2)"]},{"cell_type":"markdown","metadata":{"id":"7t9vZ6idK66i"},"source":["# Pre-processing"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3916,"status":"ok","timestamp":1641570600274,"user":{"displayName":"Nap Lan Dau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgyLJ7xblBnvLEyFishN4Q4xZSVXTxeaLzYRJU=s64","userId":"01595996840706054717"},"user_tz":-420},"id":"yuC4Y4GKK8zf","outputId":"341e3eb4-0a3e-42df-f686-25ab6f8f04ba"},"outputs":[],"source":["def unicode(data):\n","  for i in range(len(data)):\n","    data['content'][i] = data['content'][i].replace(\"òa\", \"oà\")\n","    data['content'][i] = data['content'][i].replace(\"óa\", \"oá\")\n","    data['content'][i] = data['content'][i].replace(\"ỏa\", \"oả\")\n","    data['content'][i] = data['content'][i].replace(\"õa\", \"oã\")\n","    data['content'][i] = data['content'][i].replace(\"ọa\", \"oạ\")\n","    data['content'][i] = data['content'][i].replace(\"òe\", \"oè\")\n","    data['content'][i] = data['content'][i].replace(\"óe\", \"oé\")\n","    data['content'][i] = data['content'][i].replace(\"ỏe\", \"oẻ\")\n","    data['content'][i] = data['content'][i].replace(\"õe\", \"oẽ\")\n","    data['content'][i] = data['content'][i].replace(\"ọe\", \"oẹ\")\n","    data['content'][i] = data['content'][i].replace(\"ùy\", \"uỳ\")\n","    data['content'][i] = data['content'][i].replace(\"úy\", \"uý\")\n","    data['content'][i] = data['content'][i].replace(\"ủy\", \"uỷ\")\n","    data['content'][i] = data['content'][i].replace(\"ũy\", \"uỹ\")\n","    data['content'][i] = data['content'][i].replace(\"ụy\", \"uỵ\")\n","    data['content'][i] = data['content'][i].replace(\"Ủy\", \"Uỷ\")\n","  return data\n","def replace(text, pattern, replacement, pos):\n","    matches = [0]\n","\n","    def capture_and_replace(match, ret):\n","        matches.extend([match.start() + 1, match.end()])\n","        return ret\n","\n","    l = len(text)\n","    text = re.sub(pattern, lambda match: capture_and_replace(match, replacement), text, flags=re.IGNORECASE)\n","    matches.append(l)\n","    slices = np.array_split(matches, int(len(matches) / 2))\n","    res = []\n","    for s in slices:\n","        res += pos[s[0]:s[1]]\n","    assert len(text) == len(res)\n","    return text, res\n","def preprocess(text, pos):\n","    \n","    # collapse duplicated punctuations \n","    punc = ',. !?\\\"\\''\n","    for c in punc:\n","        pat = '([' + c + ']{2,})'\n","        text, pos = replace(text, pat, c, pos)\n","    assert len(text) == len(pos)\n","    return text, pos\n","\n","test = unicode(test)\n","train = unicode(train)\n","dev = unicode(dev)"]},{"cell_type":"markdown","metadata":{"id":"VvaXLDO6PIcA"},"source":["# Tokeniner"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZqnC7MwaqV7_"},"outputs":[],"source":["df_test = pd.DataFrame()\n","df_test['spans'] = test['index_spans']\n","df_test['text'] = test['content']\n","df_test.to_csv('df_test.csv')\n","\n","df_train = pd.DataFrame()\n","df_train['spans'] = train['index_spans']\n","df_train['text'] = train['content']\n","df_train.to_csv('df_train.csv')\n","\n","df_dev = pd.DataFrame()\n","df_dev['spans'] = dev['index_spans']\n","df_dev['text'] = dev['content']\n","df_dev.to_csv('df_dev.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pnL6rnlKHsik"},"outputs":[],"source":["from vncorenlp import VnCoreNLP\n","annotator = VnCoreNLP(\"/content/vncorenlp/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx500m')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1641180328015,"user":{"displayName":"Lưu Đức Cảnh","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15467637707317635561"},"user_tz":-420},"id":"DtPB0PVULquy","outputId":"899752fe-72fb-41b1-9474-9a71e689d496"},"outputs":[],"source":["text = test['content'][325]\n","text = 'Nham quá'\n","annotator_text = annotator.tokenize(text)\n","tokens = []\n","for i in range(len(annotator_text)):\n","  for j in range(len(annotator_text[i])):\n","    tokens.append(annotator_text[i][j])\n","tokens"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OVSlJYNRMvuf"},"outputs":[],"source":["import more_itertools as mit\n","from itertools import groupby\n","from operator import itemgetter\n","def find_ranges(span):\n","  ranges =[]\n","  for k,g in groupby(enumerate(span),lambda x:x[0]-x[1]):\n","      group = (map(itemgetter(1),g))\n","      group = list(map(int,group))\n","      ranges.append((group[0],group[-1]))\n","  return ranges\n","def tokenize_word(text, pos):\n","    annotator_text = annotator.tokenize(text)\n","    tokens = []\n","    for i in range(len(annotator_text)):\n","      for j in range(len(annotator_text[i])):\n","        tokens.append(annotator_text[i][j])\n","\n","    alignment = []\n","    start = 0\n","    for t in tokens:\n","      if t == \"_\":    \n","        res = text.find(t, start)\n","        alignment.append(pos[res:res + len(t)])\n","        start = res + len(t)\n","      elif t.find(\"_.\", 0) != -1 or t.find(\"_-\", 0) != -1:       # k xuất hiện _.\n","        t = t.replace(\"_\",\"\")\n","        res = text.find(t, start)\n","        alignment.append(pos[res:res + len(t)])\n","        start = res + len(t)\n","      else:\n","        t = t.lstrip(\"_\")\n","        t = t.replace(\"_\", \" \")\n","        res = text.find(t, start)\n","        alignment.append(pos[res:res + len(t)])\n","        start = res + len(t)       \n","    assert len(tokens) == len(alignment)\n","    return tokens, alignment\n","def annotate(spans, alignment, tokens):\n","  Tag = []\n","  annotations = pd.DataFrame()\n","  annotations['Tokens'] = tokens\n","  for i in range(len(tokens)):\n","    Tag.append(\"O\")\n","  annotations['Tag'] = Tag\n","  for span in spans:\n","      i = 0\n","      while i < len(alignment):\n","          if alignment[i][-1] < span[0]:\n","            i += 1\n","          elif alignment[i][0] <= span[0] <= alignment[i][-1]:\n","              annotations['Tag'][i] = ('B-T')\n","              i += 1\n","          elif span[0] < alignment[i][0] <= span[-1]:\n","              annotations['Tag'][i] = ('I-T')\n","              i += 1\n","          elif alignment[i][0] > span[-1]:\n","              break\n","  return annotations['Tag']\n","def load_data(path):\n","  tsd = pd.read_csv(path)\n","  tsd.spans = tsd.spans.apply(literal_eval)\n","  data = []\n","  for row in tsd.iterrows():\n","      span = row[1]['spans']\n","      text = row[1]['text']\n","      temp = []\n","      text_spans = []\n","      if span:\n","        segments = list(find_ranges(span))\n","        for seg in segments:\n","          if ((len(seg) == 2) and seg[0] == seg[1]):\n","            temp.append([seg[0]])\n","            text_spans.append(text[seg[0]: seg[-1] + 1])\n","          else:\n","            temp.append([seg[0], seg[-1]])\n","            text_spans.append(text[seg[0]: seg[-1] + 1])\n","      data.append({'text': text, 'spans': temp, 'text_spans': text_spans}) \n","  return data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fia3UwtPpNxc"},"outputs":[],"source":["test_data = load_data('df_test.csv')\n","train_data = load_data('df_train.csv')\n","dev_data = load_data('df_dev.csv')\n"]},{"cell_type":"markdown","metadata":{"id":"GraLvJq-sodS"},"source":["# Annotate BIO"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wpXlKvDxvjoX"},"outputs":[],"source":["def annotate(spans, alignment, tokens):\n","  Tag = []\n","  annotations = pd.DataFrame()\n","  annotations['Tokens'] = tokens\n","  for i in range(len(tokens)):\n","    Tag.append(\"O\")\n","  annotations['Tag'] = Tag\n","  for span in spans:\n","      i = 0\n","      while i < len(alignment):\n","          if alignment[i][-1] < span[0]:\n","            i += 1\n","          elif alignment[i][0] <= span[0] <= alignment[i][-1]:\n","              annotations['Tag'][i] = ('B-T')\n","              i += 1\n","          elif span[0] < alignment[i][0] <= span[-1]:\n","              annotations['Tag'][i] = ('I-T')\n","              i += 1\n","          elif alignment[i][0] > span[-1]:\n","              break\n","  return annotations['Tag']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IakT6XgjqMnc"},"outputs":[],"source":["def data_BIO(data):\n","  formated_data = []\n","  for d in data:\n","    text = d['text']\n","    pos = [i for i in range(len(text))]\n","    text, pos = preprocess(text, pos)\n","    tokens, alignment = tokenize_word(text, pos)\n","    annotations = annotate(d['spans'], alignment, tokens)\n","    ls = [[tokens[i], annotations[i]] for i in range(len(tokens))]\n","    \n","    formated_data.extend(ls)\n","    formated_data.append([None])\n","  df_final =  pd.DataFrame(formated_data, columns= ['Word', 'Tag'])\n","  sentence_id = []\n","  sentence = 0\n","  for i in range(len(df_final)):\n","    if(df_final['Word'][i] != None):\n","      sentence_id.append(sentence)\n","    else:\n","      sentence_id.append(np.nan)\n","      sentence += 1\n","  df_final['sentence_id'] = sentence_id\n","  df_final.dropna(inplace=True)\n","  df_final['sentence_id'] = df_final['sentence_id'].astype(\"int64\")\n","  return df_final\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":21204,"status":"ok","timestamp":1641570640444,"user":{"displayName":"Nap Lan Dau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgyLJ7xblBnvLEyFishN4Q4xZSVXTxeaLzYRJU=s64","userId":"01595996840706054717"},"user_tz":-420},"id":"aHoIEsFIrAt_","outputId":"42c7b9f6-57ce-4868-8b65-362a17d88da5"},"outputs":[],"source":["test_IBO = data_BIO(test_data)\n","train_IBO = data_BIO(train_data)\n","dev_IBO = data_BIO(dev_data)\n","test_IBO"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_DJC2DoD6z9Y"},"outputs":[],"source":["test_IBO.reset_index(inplace=True)\n","dev_IBO.reset_index(inplace=True)\n","train_IBO.reset_index(inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sYxSQkjOvPGe"},"outputs":[],"source":["train_IBO.to_csv('train_BIO.csv')\n","dev_IBO.to_csv('dev_BIO.csv')\n","test_IBO.to_csv('test_BIO.csv')"]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"vscode":{"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"}}},"nbformat":4,"nbformat_minor":0}
